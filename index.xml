<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>harp</title>
    <link>https://dsc-spidal.github.io/harp/index.xml</link>
    <description>Recent content on harp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://dsc-spidal.github.io/harp/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Allgather</title>
      <link>https://dsc-spidal.github.io/harp/docs/communications/allgather/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/communications/allgather/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/3-3-1.png&#34; alt=&#34;allgather&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;allgather&lt;/code&gt; aims to first collect tables from other workers and then broadcast the collection. All workers should run it concurrently. The defination of the method is:&lt;/p&gt;
boolean allgather(final String contextName, final String operationName, final Table&lt;P&gt; table, final DataMap dataMap, final Workers workers)

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;contextName&lt;/code&gt; &amp;mdash; the name of the context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;operationName&lt;/code&gt; &amp;mdash; the name of the operation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;table&lt;/code&gt; &amp;mdash; the name of the data table&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataMap&lt;/code&gt; &amp;mdash; the data map&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workers&lt;/code&gt; &amp;mdash; the workers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
allgather(contextName, &#34;allgather&#34;, table, dataMap, workers);
</description>
    </item>
    
    <item>
      <title>Allreduce</title>
      <link>https://dsc-spidal.github.io/harp/docs/communications/allreduce/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/communications/allreduce/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/3-4-1.png&#34; alt=&#34;allreduce&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;allreduce&lt;/code&gt; aims to first combine tables from other workers and then broadcast the accumulated table. All workers should run it concurrently. The defination of the method is:&lt;/p&gt;
boolean allreduce(final String contextName, final String operationName, final Table&lt;P&gt; table, final DataMap dataMap, final Workers workers)

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;contextName&lt;/code&gt; &amp;mdash; the name of the context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;operationName&lt;/code&gt; &amp;mdash; the name of the operation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;table&lt;/code&gt; &amp;mdash; the name of the data table&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataMap&lt;/code&gt; &amp;mdash; the data map&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workers&lt;/code&gt; &amp;mdash; the workers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
allreduce(contextName, &#34;allreduce&#34;, table, dataMap, workers);
</description>
    </item>
    
    <item>
      <title>Broadcast</title>
      <link>https://dsc-spidal.github.io/harp/docs/communications/broadcast/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/communications/broadcast/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/3-1-1.png&#34; alt=&#34;broadcast&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;broadcast&lt;/code&gt; aims to share a table in one worker with others. All workers should run it concurrently. The defination of the method is:&lt;/p&gt;
boolean broadcast(String contextName, String operationName, Table&lt;P&gt; table, int bcastWorkerID, boolean useMSTBcast, DataMap dataMap, Workers workers)

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;contextName&lt;/code&gt; &amp;mdash; the name of the context&lt;/li&gt;
&lt;li&gt;&lt;code&gt;operationName&lt;/code&gt; &amp;mdash; the name of the operation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;table&lt;/code&gt; &amp;mdash; the name of the data table&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bcastWorkerID&lt;/code&gt; &amp;mdash; the ID of the worker which broadcasts&lt;/li&gt;
&lt;li&gt;&lt;code&gt;useMSTBcast&lt;/code&gt; &amp;mdash; whether use MST method or not&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataMap&lt;/code&gt; &amp;mdash; the data map&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workers&lt;/code&gt; &amp;mdash; the workers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
broadcast(contextName, &#34;chain-array-table-bcast-&#34; + i, arrTable, workers.getMasterID(), false, dataMap, workers);
</description>
    </item>
    
    <item>
      <title>Community</title>
      <link>https://dsc-spidal.github.io/harp/docs/contributors/community/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/contributors/community/</guid>
      <description>

&lt;h2 id=&#34;contributing-to-harp&#34;&gt;Contributing to Harp&lt;/h2&gt;

&lt;p&gt;Discussion about Harp happens on GitHub and over mailing list.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/DSC-SPIDAL/harp.git&#34;&gt;DSC-SPIDAL/Harp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Harp User Google Group: &lt;a href=&#34;https://groups.google.com/forum/#!forum/harp-users&#34;&gt;harp-users@googlegroups.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Community is critical to Harp. Contributions are welcomed!&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-contribute-to-harp&#34;&gt;How Can I Contribute to Harp?&lt;/h2&gt;

&lt;p&gt;You can first read the following pages to have a basic understanding
of Heron:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../../getting-started/&#34;&gt;Harp Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../programming/data-interface/&#34;&gt;Data Interfaces and Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../programming/computation-models/&#34;&gt;Computation Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In general, contributions that fix bugs or add features (as opposed to stylistic, refactoring, or
&amp;ldquo;cleanup&amp;rdquo; changes) are preferred. Please check with &lt;a href=&#34;https://groups.google.com/forum/#!forum/heron-users&#34;&gt;mailing list&lt;/a&gt;
if your patch involves lots of changes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you have any question or issues about troubleshooting,
you should post on &lt;a href=&#34;https://groups.google.com/forum/#!forum/harp-users&#34;&gt;mailing list&lt;/a&gt; instead
of opening GitHub issues.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;submitting-a-patch&#34;&gt;Submitting a Patch&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Read and accept the
&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Discuss your plan and design, and get agreement on
&lt;a href=&#34;https://groups.google.com/forum/#!forum/harp-users&#34;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Implement proper unit tests along with your change. Verify that all tests can pass.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Submit a GitHub pull request that includes your change and test cases.
Describe clearly your pull request the change.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Complete a code review by addressing reviewers&amp;rsquo;s comments.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A project committer will merge the patch to the master branch.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Data Interfaces and Types</title>
      <link>https://dsc-spidal.github.io/harp/docs/programming/data-interface/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/programming/data-interface/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/2-2-1.png&#34; alt=&#34;data-abstraction&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Harp provides three levels of data structures: arrays and objects, partition, and table. Arrays and Serializable objects are the basic data structures, which include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;ByteArray&lt;/code&gt;: an array with byte-type elements&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;ShortArray&lt;/code&gt;: an array with short-type elements&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;IntArray&lt;/code&gt;: an array with int-type elements&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;FloatArray&lt;/code&gt;: an array with float-type elements&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;LongArray&lt;/code&gt;: an array with long-type elements&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;DoubleArray&lt;/code&gt;: an array with double-type elements&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;Writable&lt;/code&gt;: serializable object&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code&gt;Partition&lt;/code&gt; is a wraper of the data structures shown above. Every partition has an ID. In collective communication, partitions from different processors with the same ID will be merged. The merge operation is defined by &lt;code&gt;PartitionCombiner&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Table&lt;/code&gt; is a container for partitions. It is a high-level data structure and the unit for collective communication.&lt;/p&gt;

&lt;h1 id=&#34;table-and-partitions&#34;&gt;Table and Partitions&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/3-1-2.png&#34; alt=&#34;table-partition&#34; /&gt;&lt;/p&gt;

&lt;p&gt;An example of how to construct a table is:&lt;/p&gt;
Table&lt;DoubleArray&gt; table = new Table&lt;&gt;(0, new DoubleArrPlus());
for (int i = 0; i &lt; numPartitions; i++) {
    DoubleArray array = DoubleArray.create(size, false);
    table.addPartition(new Partition&lt;&gt;(i, array));
}

&lt;p&gt;In this example, it initiliazes a table which carries &lt;code&gt;DoubleArray&lt;/code&gt; as the primitive data type. &lt;code&gt;DoubleArrPlus&lt;/code&gt; is a &lt;code&gt;PartitionCombiner&lt;/code&gt; used to define the merging operation of two partitions.&lt;/p&gt;
public class DoubleArrPlus extends PartitionCombiner&lt;DoubleArray&gt; {
    public PartitionStatus combine(DoubleArray curPar, DoubleArray newPar) {
        double[] doubles1 = curPar.get(); int size1 = curPar.size();
        double[] doubles2 = newPar.get(); int size2 = newPar.size();
        if (size1 != size2) {            return PartitionStatus.COMBINE_FAILED;
        }
        for (int i = 0; i &lt; size2; i++) {
            doubles1[i] = doubles1[i] + doubles2[i];
        }
        return PartitionStatus.COMBINED;
   }
}


&lt;h1 id=&#34;data-abstraction&#34;&gt;Data Abstraction&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/3-1-3.png&#34; alt=&#34;data-types&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The data abstraction is shown above. &lt;code&gt;Transferable&lt;/code&gt; is ae higher interface compare to other data structures and &lt;code&gt;Simple&lt;/code&gt; is the sub-interface for all primitive data structures. Here is an example of the primitive data strucutres.&lt;/p&gt;
/*
 * Copyright 2013-2016 Indiana University
 * 
 * Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package edu.iu.harp.resource;

import java.io.DataOutput;
import java.io.IOException;

import edu.iu.harp.io.DataType;
import edu.iu.harp.resource.Array;

/*******************************************************
 * IntArray class for managing int[] data.
 ******************************************************/
public final class IntArray extends Array&lt;int[]&gt; {

    public IntArray(int[] arr, int start, int size) {
	super(arr, start, size);
    }

    /**
     * Get the number of Bytes of encoded data. One byte for storing DataType,
     * four bytes for storing the size, size*4 bytes for storing the data.
     */
    @Override
    public int getNumEnocdeBytes() {
	return this.size * 4 + 5;
    }

    /**
     * Encode the array as DataOutput
     */
    @Override
    public void encode(DataOutput out) throws IOException {
	out.writeByte(DataType.INT_ARRAY);
	int len = start + size;
	out.writeInt(size);
	for (int i = start; i &lt; len; i++) {
	    out.writeInt(array[i]);
	}
    }

    /**
     * Create an array. Firstly try to get an array from ResourcePool; if
     * failed, new an array.
     * 
     * @param len
     * @param approximate
     * @return
     */
    public static IntArray create(int len, boolean approximate) {
	if (len &gt; 0) {
	    int[] ints = ResourcePool.get().getIntsPool().getArray(len, approximate);
	    if (ints != null) {
		return new IntArray(ints, 0, len);
	    } else {
		return null;
	    }
	} else {
	    return null;
	}
    }

    /**
     * Release the array from the ResourcePool
     */
    @Override
    public void release() {
	ResourcePool.get().getIntsPool().releaseArray(array);
	this.reset();
    }

    /**
     * Free the array from the ResourcePool
     */
    @Override
    public void free() {
	ResourcePool.get().getIntsPool().freeArray(array);
	this.reset();

    }
}

</description>
    </item>
    
    <item>
      <title>Developers and Contributors</title>
      <link>https://dsc-spidal.github.io/harp/docs/contributors/contributors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/contributors/contributors/</guid>
      <description>&lt;p&gt;Judy Qiu&lt;/p&gt;

&lt;p&gt;Bingjing Zhang&lt;/p&gt;

&lt;p&gt;Bo Peng&lt;/p&gt;

&lt;p&gt;Langshi Chen&lt;/p&gt;

&lt;p&gt;Ethan Li&lt;/p&gt;

&lt;p&gt;Yiming Zou&lt;/p&gt;

&lt;p&gt;Yining Wang&lt;/p&gt;

&lt;p&gt;Abby Kaufman&lt;/p&gt;

&lt;p&gt;Anchal Khandelwal&lt;/p&gt;

&lt;p&gt;Gowtham Ayna Raveendran&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Examples Overview</title>
      <link>https://dsc-spidal.github.io/harp/docs/examples/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/examples/overview/</guid>
      <description>

&lt;p&gt;This section has six tutorials demonstrating how to implement distributed machine learning algorithms with Harp framework.&lt;/p&gt;

&lt;p&gt;Modern CPU and computation devices, such as many-core and GPU, provide powerful computing capacity but are usually challenging to deploy efficiently. To make it easier to design and implement distributed machine learning algorithms, we adopt a systematic process of parallelization with a focus on the computation models and their communication mechanisms. The tutorials provide examples of broadly used machine algorithms, including K-means, Multi-class Logistic Regression, Random Forest, Support Vector Machine, Latent Dirichlet Allocation and Neural Network, to represent the basics idea and steps for programming that port a non-trivial analysis algorithm from a sequential code into a distributed version.&lt;/p&gt;

&lt;p&gt;These examples focus on the parallelization concept and expressiveness using Harp API, rather than performance optimization. In order to explore real applications, further tuning and advanced optimizations are necessary, which have been demonstrated in to the next section of real applications.&lt;/p&gt;

&lt;p&gt;There are some general concepts that should be introduced before the tutorial starts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Input: Input data feed into the training process of the machine learning algorithm, which are also called &amp;lsquo;training data&amp;rsquo;, &amp;lsquo;data points&amp;rsquo;,&amp;lsquo;examples&amp;rsquo; or &amp;lsquo;instances&amp;rsquo;. Normally, input data are large and partitioned among the machines(nodes) in the cluster, calls &amp;lsquo;input splits&amp;rsquo;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Model: The “model” is the output generated when you train your “machine learning algorithm” with your training data-set. Here we focus on the data part.
Training: Machine learning algorithms are normally iterative computation, processing the training data and update the model in each iteration and stop when reach the stop criterion.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Prediction: Use the model learned from training data-set, and apply new data on it to get the outputs, the predictions.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Data Parallelism: Training data are partitioned among nodes and all the splits are processed in parallel. It&amp;rsquo;s essential for big data problem.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Model Parallelism: Model data are partitioned among nodes and model updates on each split are processed in parallel. It&amp;rsquo;s essential for big model problem.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The tutorial follows a similar structure, with 4 main sections:&lt;/p&gt;

&lt;h2 id=&#34;1-understanding-algorithm&#34;&gt;1. Understanding Algorithm&lt;/h2&gt;

&lt;p&gt;This part gives simple background information on the target machine learning algorithm itself. The original algorithm does not need to have parallelism in consideration.&lt;/p&gt;

&lt;h2 id=&#34;2-parallel-design&#34;&gt;2. Parallel Design&lt;/h2&gt;

&lt;p&gt;This part illustrates the process of how to analysis the original sequential algorithm and to utilize the intrinsic parallelisms to design a parallel algorithm.&lt;/p&gt;

&lt;p&gt;Under the Map-Collective programming model in Harp framework, there is a general pattern to do the parallel design for machine learning algorithms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/4-1-1.png&#34; alt=&#34;Overview-1&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;figure-map-collective-programming-model-on-iterative-machine-learning-algorithms&#34;&gt;Figure Map-Collective Programming Model on Iterative Machine Learning Algorithms&lt;/h5&gt;

&lt;p&gt;For training data, Harp will load the local data split on each node into memory in the initialization step of training, with no disk I/O to access the training data in the future. By default, the data split mechanism support by Hadoop Mapreduce are used.&lt;/p&gt;

&lt;p&gt;For model data, Harp provides distributed dataset abstractions and collective communication and synchronization operations. Since the core computation of machine learning algorithms lies in the model update, the problems of model consistency and synchronization arise when parallelizing the core computation of model update. Harp has unique abstractions built upon collective synchronization mechanism, which is advantageous in expressiveness, efficiency and effectiveness for distributed machine learning applications.&lt;/p&gt;

&lt;p&gt;The standard steps aim to answer four questions about the model design for a distributed machine learning algorithm based on its sequential algorithm.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What is the model? What kind of data structure that is applicable?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What are the characteristics of the data dependency in model update computation – can they run concurrently?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Which parallelism scheme is suitable – data parallelism or model parallelism?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Which collective communication operation is optimal to synchronize the model?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-code-and-comments&#34;&gt;3. Code and Comments&lt;/h2&gt;

&lt;p&gt;The code snippets and comments illustrate the details of parallel implementations.&lt;/p&gt;

&lt;h2 id=&#34;4-run-demo&#34;&gt;4. Run Demo&lt;/h2&gt;

&lt;p&gt;Following the command or scripts, user can try the tutorial examples on a dataset by themselves.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Harp Computation Models</title>
      <link>https://dsc-spidal.github.io/harp/docs/programming/computation-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/programming/computation-models/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/2-4-1.png&#34; alt=&#34;Inter-node Computation Model&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;computation-model-a-locking-use-the-synchronized-algorithm-and-the-latest-model-parameters&#34;&gt;Computation Model A (Locking, use the synchronized algorithm and the latest model parameters)&lt;/h2&gt;

&lt;p&gt;This “Locking”-based computation model guarantees each worker the exclusive access to model parameters. Once a worker trains a data item, it locks the related model parameters and prevents other workers from accessing them. When the related model parameters are updated, the worker unlocks the parameters. Thus, the model parameters used in local computation is always the latest. This computation model can be implemented through Harp event-driven APIs.&lt;/p&gt;

&lt;h2 id=&#34;computation-model-b-rotation-use-the-synchronized-algorithm-and-the-latest-model-parameters&#34;&gt;Computation Model B (Rotation, use the synchronized algorithm and the latest model parameters)&lt;/h2&gt;

&lt;p&gt;The second model is a “Rotation”-based computation model that rotates model parameters between workers. Each worker first takes a part of the shared model and performs training. Then, the model is shifted between the workers. Through model rotation, each model parameters are updated by one worker at a time so that the model is consistent. This computation model can be implemented with Harp “rotate” operation.&lt;/p&gt;

&lt;h2 id=&#34;computation-model-c-allreduce-use-the-synchronized-algorithm-and-the-stale-model-parameters&#34;&gt;Computation Model C (Allreduce, use the synchronized algorithm and the stale model parameters)&lt;/h2&gt;

&lt;p&gt;In this computation model, each process first fetches all the model parameters required by local computation. When the local computation is completed, modifications of the local model from all processes are gathered to update the model. This computation model can be implemented through either the “allreduce” operation for small models, the “regroup+allgather” operation or “psuh&amp;amp;pull” with big models.&lt;/p&gt;

&lt;h2 id=&#34;computation-model-d-no-sync-use-the-asynchronous-algorithm-and-the-stale-model-parameters&#34;&gt;Computation Model D (No-sync, use the asynchronous algorithm and the stale model parameters)&lt;/h2&gt;

&lt;p&gt;For the last computation model, each process independently fetches related model parameters, performs local computation, and returns model modifications. Unlike the “Locking”-based computation model, workers are allowed to fetch or update the same model parameters in parallel. In contrast to the “Rotation” and the “Allreduce” computation models, there is no synchronization barrier. This computation model can be implemented through Harp event-driven APIs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Harp Multiclass Logistic Regression with Stochastic Gradient Descent</title>
      <link>https://dsc-spidal.github.io/harp/docs/examples/mlrsgd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/examples/mlrsgd/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/4-3-1.png&#34; width=&#34;60%&#34;  &gt;&lt;/p&gt;

&lt;p&gt;Multiclass logistic regression (MLR) is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables.&lt;/p&gt;

&lt;p&gt;The process of the MLR algorithm is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Use the weight &lt;code&gt;W&lt;/code&gt; to predict the label of current data point.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Compare the output and the answer.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use SGD to approximate &lt;code&gt;W&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat step 1 to 3 with each label and their weights.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Stochastic gradient descent (SGD) is a stochastic approximation of the gradient descent optimization method for minimizing an objective function that is written as a sum of differentiable functions. In other words, SGD tries to find minimums or maximums by iteration. As the algorithm sweeps through the training set, it performs the update for each training example. Several passes can be made over the training set until the algorithm converges.&lt;/p&gt;

&lt;p&gt;The SGD algorithm can be described as following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Randomly assign the weight &lt;code&gt;W&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Shuffle &lt;code&gt;N&lt;/code&gt; data points.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Go through &lt;code&gt;N&lt;/code&gt; data points and do gradient descent.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat step 2 and 3 &lt;code&gt;K&lt;/code&gt; times.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Definitions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;N&lt;/code&gt; is the number of data points&lt;/li&gt;
&lt;li&gt;&lt;code&gt;T&lt;/code&gt; is the number of labels&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M&lt;/code&gt; is the number of features&lt;/li&gt;
&lt;li&gt;&lt;code&gt;W&lt;/code&gt; is the &lt;code&gt;T*M&lt;/code&gt; weight matrix&lt;/li&gt;
&lt;li&gt;&lt;code&gt;K&lt;/code&gt; is the number of iteration&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;parallel-design&#34;&gt;PARALLEL DESIGN&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What are the model? What kind of data structure?&lt;/p&gt;

&lt;p&gt;The weight vectors for classes are model. Because an ovr(one-versus-rest) approach is adopted, each weight vector are independent. It has a matrix structure.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What are the characteristics of the data dependency in model update computation, can updates run concurrently?&lt;/p&gt;

&lt;p&gt;Model update computation here is the SGD update, in which for each data point it should update the model directly. Because of the ovr strategy, each row in the model matrix are independent, and can be updated in parallel.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;which kind of parallelism scheme is suitable, data parallelism or model parallelism?&lt;/p&gt;

&lt;p&gt;Data parallelism can be used, i.e., calculating different data points in parallel.&lt;/p&gt;

&lt;p&gt;Because the updates can run concurrently, model parallelism is a nature solution.  Each node get one partition of the model, which updates in parallel. And furthermore, thread level parallelism can also follows this model parallelism pattern, that each thread take a subset of partition and update in parallel independently.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;which collective communication operations is suitable to synchronize model?&lt;/p&gt;

&lt;p&gt;DynamicScheduler can be used for thread-level parallelism, and Rotate can be used in the inter-node model synchronization.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dataflow&#34;&gt;DATAFLOW&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/4-3-2.png&#34; alt=&#34;dataflow&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;step-0-data-preprocessing&#34;&gt;Step 0 &amp;mdash; Data preprocessing&lt;/h2&gt;

&lt;p&gt;Harp MLR will use the data in the vector format. Each vector in a file represented by the format &lt;code&gt;&amp;lt;did&amp;gt; [&amp;lt;fid&amp;gt;:&amp;lt;weight&amp;gt;]&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;did&amp;gt;&lt;/code&gt; is an unique document id&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;fid&amp;gt;&lt;/code&gt; is a positive feature id&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;weight&amp;gt;&lt;/code&gt; is the number feature value within document weight&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After preprocessing, push the data set into HDFS by the following commands.&lt;/p&gt;
hdfs dfs -mkdir /input
hdfs dfs -put input_data/* /input

&lt;h2 id=&#34;step-1-initialize&#34;&gt;Step 1 &amp;mdash; Initialize&lt;/h2&gt;

&lt;p&gt;For Harp MLR, we will use dynamic scheduling as mentioned above. Before we set up the dynamic scheduler, we need to initialize the weight matrix &lt;code&gt;W&lt;/code&gt;, which will be partitioned into &lt;code&gt;T&lt;/code&gt; parts representing to &lt;code&gt;T&lt;/code&gt; labels which means that each label belongs to one partition and is treated as an independent task.&lt;/p&gt;
private void initTable() {
    wTable = new Table(0, new DoubleArrPlus());
    for (int i = 0; i &lt; topics.size(); i++)
        wTable.addPartition(new Partition(i, DoubleArray.create(TERM + 1, false)));
}

&lt;p&gt;After that we can initialize the dynamic scheduler. Each thread will be treated as a worker and be added into the scheduler. The only thing that needs to be done is that tasks has to be submitted during the computation.&lt;/p&gt;
private void initThread() {
    GDthread = new LinkedList&lt;&gt;();
    for (int i = 0; i &lt; numThread; i++)
        GDthread.add(new GDtask(alpha, data, topics, qrels));
    GDsch = new DynamicScheduler&lt;&gt;(GDthread);
}

&lt;h2 id=&#34;step-2-mapper-communication&#34;&gt;Step 2 &amp;mdash; Mapper communication&lt;/h2&gt;

&lt;p&gt;In this main process, we use &lt;code&gt;regroup&lt;/code&gt; to distribute the partitions to the workers first. The workers will get almost the same number of partitions. Then we start the scheduler. For each time we submit one partition to each thread in the scheduler and the threads will all use SGD to approximate &lt;code&gt;W&lt;/code&gt; with each label. After the workers finish once with their own partitions, we will use &lt;code&gt;rotate&lt;/code&gt; operation to swap the partitions among the workers. When finishing the all process, each worker should use its own data training the whole partition &lt;code&gt;K&lt;/code&gt; times, of which &lt;code&gt;K&lt;/code&gt; is the number of iteration. &lt;code&gt;allgather&lt;/code&gt; operation collects all partitions in each worker, combines the partitions, and shares the outcome with all workers. Finally, the Master worker outputs the weight matrix &lt;code&gt;W&lt;/code&gt;.&lt;/p&gt;
protected void mapCollective(KeyValReader reader, Context context) throws IOException, InterruptedException {
    LoadAll(reader);
    initTable();
    initThread();

    regroup(&#34;MLR&#34;, &#34;regroup_wTable&#34;, wTable, new Partitioner(getNumWorkers()));

    GDsch.start();        
    for (int iter = 0; iter &lt; ITER * numMapTask; iter++) {
        for (Partition par : wTable.getPartitions())
            GDsch.submit(par);
        while (GDsch.hasOutput())
            GDsch.waitForOutput();
            
        rotate(&#34;MLR&#34;, &#34;rotate_&#34; + iter, wTable, null);

        context.progress();
    }
    GDsch.stop();
        
    allgather(&#34;MLR&#34;, &#34;allgather_wTable&#34;, wTable);

    if (isMaster())
        Util.outputData(outputPath, topics, wTable, conf);
    wTable.release();
}

&lt;h2 id=&#34;usage&#34;&gt;USAGE&lt;/h2&gt;
$ hadoop jar harp-tutorial-app-1.0-SNAPSHOT.jar edu.iu.mlr.MLRMapCollective [alpha] [number of iteration] [number of features] [number of workers] [number of threads] [topic file path] [qrel file path] [input path in HDFS] [output path in HDFS]
#e.g. hadoop jar harp-tutorial-app-1.0-SNAPSHOT.jar edu.iu.mlr.MLRMapCollective 1.0 100 47236 2 16 /rcv1v2/rcv1.topics.txt /rcv1v2/rcv1-v2.topics.qrels /input /output

&lt;p&gt;The output should be the weight matrix &lt;code&gt;W&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Harp Neural Network</title>
      <link>https://dsc-spidal.github.io/harp/docs/examples/nn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/examples/nn/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/nn.png&#34; width=&#34;60%&#34;  &gt;&lt;/p&gt;

&lt;p&gt;Neural networks are a set of algorithms, which is based on a large of neural units. Each neural unit is connected with many others, and forms a network structure. Computation happens in the neural unit, which combines all the inputs with a set of coefficients, or weights, and gives an output by an activation function. A layer is a group of neural units, that each layer’s output is the subsequent layer’s input. A learning algorithm tries to learn the weights from data, and then the network can be used to recognize patterns.&lt;/p&gt;

&lt;p&gt;Here, we give a simple tutorial on how to parallel a standard implementation of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Backpropagation&#34;&gt;BP algorithm&lt;/a&gt; for a feed-forward network.&lt;/p&gt;

&lt;h2 id=&#34;parallel-design&#34;&gt;PARALLEL DESIGN&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What are the model? What kind of data structure?&lt;/p&gt;

&lt;p&gt;Weights matrices, including the biases for each node, between each adjacent layers are the model in neural network. It is a vector of double matrix.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;What are the characteristics of the data dependency in model update computation, can updates run concurrently?&lt;/p&gt;

&lt;p&gt;In the core model update computation in BP training algorithm, each data point, or a minibatch, should access all the model, compute gradients and update model layer by layer from the output layer back to the input layer.&lt;/p&gt;

&lt;p&gt;The nodes in the same layer can be updated in parallel without conflicts, but there are dependency between the layers. But generally, it is not easy to utilize these network structure related parallelism.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;which kind of parallelism scheme is suitable, data parallelism or model parallelism?&lt;/p&gt;

&lt;p&gt;Data parallelism can be used, i.e., calculating different data points in parallel.&lt;/p&gt;

&lt;p&gt;No model parallelism, each node get one replica of the whole model, which updates locally in parallel, and then synchronizes and averages when local computation all finish.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;which collective communication operations is suitable to synchronize model?&lt;/p&gt;

&lt;p&gt;Synchronize replicas of the model by allreduce is an simple solution.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dataflow&#34;&gt;DATAFLOW&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/nn-dataflow.png&#34; width=&#34;60%&#34;  &gt;&lt;/p&gt;

&lt;h2 id=&#34;step-1-set-table&#34;&gt;Step 1 &amp;mdash; Set Table&lt;/h2&gt;

&lt;p&gt;The data format wrapper code is in charge of the conversion between the native DoubleMatrix and Harp Table.&lt;/p&gt;
public Table&lt;DoubleArray&gt; train(DoubleMatrix X, DoubleMatrix Y, Table&lt;DoubleArray&gt; localWeightTable, int mini_epochs, int numMapTasks, double lambda) throws IOException
{
    Vector&lt;DoubleMatrix&gt; weightMatrix = reshapeTableToList(localWeightTable);
    setTheta(weightMatrix);

    trainBP(X, Y, lambda, mini_epochs, true);

    Table&lt;DoubleArray&gt; newWeightTable = reshapeListToTable(this.getTheta());
    return newWeightTable;
}

&lt;h2 id=&#34;step-2-communication&#34;&gt;Step 2 &amp;mdash;Communication&lt;/h2&gt;

&lt;p&gt;The code snippet for the core part of computation in the iterative training. There  are only a few lines of differences between the harp distributed version and the original sequential version.&lt;/p&gt;
// Calculate the new weights 
// argument data type conversion and call the train() in the underlie library
weightTable = localNN.train(X, Y, weightTable, n, numMapTasks, lambda);

// reduce and broadcast
allreduce(&#34;main&#34;, &#34;allreduce&#34; + i, weightTable);

// Average the weight table by the numMapTasks
weightTable = localNN.modelAveraging(weightTable, numMapTasks);

&lt;h2 id=&#34;data&#34;&gt;DATA&lt;/h2&gt;

&lt;p&gt;The MNIST dataset is used in this tutorial. Refer the &lt;a href=&#34;https://github.com/DSC-SPIDAL/harp/tree/master/data/tutorial/mnist&#34;&gt;dataset script&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&#34;usage&#34;&gt;USAGE&lt;/h2&gt;
$ hadoop jar harp-tutorial-app-1.0-SNAPSHOT.jar edu.iu.NN.NNMapCollective
Usage: NNMapCollective &lt;number of map tasks&gt; &lt;epochs&gt; &lt;syncIterNum&gt; &lt;hiddenLayers&gt; &lt;minibatchsize&gt; &lt;lambda&gt; &lt;workDir&gt;
# hadoop jar harp-tutorial-app-1.0-SNAPSHOT.jar edu.iu.NN.NNMapCollective 2 20 5 100,32 2000 0 /nn

&lt;p&gt;This command run harp neuralnetwork training on the input dataset under /nn, with 2 mappers. Training process goes through 20 times of the training dataset, averages the model every 5 iteration for each minibatch. The minibatch size is 2000, lambda is default value 0.689. There are 2 hidden layers, with 100 and 32 nodes each. Finally, it outputs the accuracy on the training set into the hadoop log.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Harp Overview</title>
      <link>https://dsc-spidal.github.io/harp/docs/programming/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/programming/overview/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/harp-overview.png&#34; alt=&#34;Overview-0&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;big-model-problems-and-the-limitation-of-existing-tools&#34;&gt;Big Model Problems and The Limitation of Existing Tools:&lt;/h2&gt;

&lt;p&gt;Data analytics is undergoing a revolution in many scientific domains. Machine learning becomes a popular method for analytics for which it allows computers to learn from the existing data and make predictions based off it. They have been widely used in computer vision, text mining, advertising, recommender systems, network analysis and genetics. Unfortunately, analyzing such huge data usually exceeds the capability of a single or even a few machines owing to the incredible volume of data available, and thus requires algorithm parallelization at an unprecedented scale. Scaling up these algorithms is challenging because of their prohibitive computation cost, not only the need to process enormous training data in iterations, but also the requirement to synchronize big model in rounds for algorithm convergence. The problem is simply referred as &amp;ldquo;the big model problem of big data machine learning&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Many machine learning algorithms were implemented in MapReduce. However, these implementations suffer from repeated input data loading from the distributed file systems and slow disk-based intermediate data synchronization in the shuffling phase. This motivates the design of iterative MapReduce tools which utilize memory for data caching and communication and thus drastically improve the performance of large-scale data processing. Later big data tools have expanded rapidly and form an open-source software stack. Their programming models are not limited to MapReduce and iterative MapReduce. In graph processing tools, input data are abstracted as a graph and processed in iterations, while intermediate data per iteration are expressed as messages transmitted between vertices. In parameter servers, model parameters are stored in a set of server machines and they can be retrieved asynchronously in parallel processing.&lt;/p&gt;

&lt;p&gt;While in contemporary tools performance is improved with in-memory caching, observations show that the parallelization of these iterative applications still suffers from two issues. To simplify the programming process, many tools’ design tries to fix the parallel execution flow and developers are only required to fill the bodies of user functions. However this results in limited support of the synchronization patterns. The parallelization performance suffers from performance inefficiency due to in-proper usage of synchronization patterns. To avoid this issue, some work turn to use MPI to develop machine learning applications. However, these applications developed achieve high performance but fall into the complicated code bases since MPI only provides basic communication operations.&lt;/p&gt;

&lt;h2 id=&#34;harp-highlights&#34;&gt;Harp Highlights:&lt;/h2&gt;

&lt;p&gt;To solve the problems mentioned above, the Harp’s approach is to use collective communication techniques to improve the performance of model synchronization in parallel machine learning. Therefore a MapCollective programming model is extended from the original MapReduce programming model. Similar to the MapReduce model, the MapCollective model still read key-value pairs as inputs. However, instead of using the shuffling phase, Harp uses optimized collective communication operations for data movement and provide high-level interfaces with partitioned distributed dataset abstractions for various synchronization patterns in iterative machine learning computation. These enhancements are designed as plug-ins to Hadoop so Harp can enrich the whole big data software stack.&lt;/p&gt;

&lt;p&gt;With the Harp framework, the project focuses on building a machine learning library with the programming interfaces provided. Our research shows parallel machine learning applications can be categorized to four types of computation models. The classification of the computation models is based on the synchronization patterns and the effectiveness of the model parameter update. These computation models are mapped to the Harp programming interfaces to simplify the programming of machine learning applications. In sum, the Harp’s contribution includes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Harp provides a collective communication library as a Hadoop plug-in and a set of Map-Collective programming interfaces to develop iterative machine learning applications with various synchronization patterns.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Four parallel computation models are categorized to characterize the parallelization of machine learning applications. “Allreduce” and “Rotation” based Computation models can be mapped to the Harp collective communication interfaces in order to simplify the implementation of parallel machine learning applications.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A collection of machine learning algorithms are implemented, including K-means Clustering, Multiclass Logistic Regression (MLR), Support Vector Machine (SVM), Latent Dirichlet Allocation (LDA) and Matrix Factorization (MF).  Our experiment results of LDA implementations reveal that the &amp;ldquo;Rotation&amp;rdquo; based computation model is faster than the “Allreduce” type computation model. Now three algorithms are built on top of model rotation: Collapsed Gibbs Sampling (CGS) for LDA, Stochastic Gradient Descent (SGD) and Cyclic Coordinate Descent (CCD) for MF. The performance results on an Intel Haswell cluster show that our solution achieves faster model convergence speed and higher scalability compared with other related work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Harp Random Forests</title>
      <link>https://dsc-spidal.github.io/harp/docs/examples/rf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/examples/rf/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/4-5-1.png&#34; width=&#34;60%&#34;  &gt;&lt;/p&gt;

&lt;p&gt;Random forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees&amp;rsquo; habit of overfitting to their training set.&lt;/p&gt;

&lt;p&gt;The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set &lt;code&gt;X&lt;/code&gt; with responses &lt;code&gt;Y&lt;/code&gt;, bagging repeatedly selects a random sample with replacement of the training set. After training, predictions for unseen samples can be made by averaging the predictions from all the individual regression trees or by taking the majority vote in the case of decision trees. Random forests can also do another bagging &amp;ndash; feature bagging, which is a random subset of the features. The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated.&lt;/p&gt;

&lt;h2 id=&#34;definition&#34;&gt;DEFINITION&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;N&lt;/code&gt; is the number of data points&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M&lt;/code&gt; is the number of selected features&lt;/li&gt;
&lt;li&gt;&lt;code&gt;K&lt;/code&gt; is the number of trees&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;method&#34;&gt;METHOD&lt;/h2&gt;

&lt;p&gt;The following is the procedure of Harp Random Forests training:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Randomly select &lt;code&gt;N&lt;/code&gt; samples with replacement for &lt;code&gt;K&lt;/code&gt; trees (totally &lt;code&gt;K&lt;/code&gt; times).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select &lt;code&gt;M&lt;/code&gt; features randomly which will be used in decision tree.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Build decision tree based on these &lt;code&gt;M&lt;/code&gt; features&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat Step 2 and 3 &lt;code&gt;K&lt;/code&gt; times.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The predicting part will use these decision trees to predict &lt;code&gt;K&lt;/code&gt; results. For regression, the result is the average of each tree&amp;rsquo;s result and for classification, the majority vote will be the output.&lt;/p&gt;

&lt;h2 id=&#34;step-0-data-preprocessing&#34;&gt;Step 0 &amp;mdash; Data preprocessing&lt;/h2&gt;
// In the case of merging the data from different location, main process needs to create bootstrap samples.
// So here, the main process loads all the data and creates the bootstrap samples.
if(doBootstrapSampling) {                                     
    //Load each of the training files under train folder                
    for(i = 0; i &lt; RandomForestConstants.NUM_GLOBAL; i++) {
        files.add(trainFileFolder + File.separator + trainNameFormat + i + &#34;.csv&#34;);
    }
    System.out.println(&#34;Loading the training data...&#34;);
    loadData(trainData,files);
    System.out.println(&#34;Data size: &#34; + trainData.size());
    files.clear();
    ArrayList&lt;Integer&gt; positions;
    //Create the bootstrapped samples and write to local disk
    for(i = 0; i &lt; RandomForestConstants.NUM_MAPPERS; i ++) {
        positions = DoBootStrapSampling(trainData.size());
        System.out.println(&#34;Sampled data for: &#34;+ i +&#34;; size: &#34; + positions.size());
        createMapperFiles(trainData,testData,fs,localDirStr,i,positions);
        positions = null;
    }
} else {
    //Load each of the training files under train folder                
    for(i = 0; i &lt; RandomForestConstants.NUM_MAPPERS; i ++){
        System.out.println(&#34;Loading the training data for &#34; + i + &#34;...&#34;);
        files.add(trainFileFolder+File.separator+trainNameFormat+i+&#34;.csv&#34;);
        loadData(trainData,files);
        System.out.println(&#34;Unsampled data for: &#34;+ i +&#34;; size: &#34; + trainData.size());
        createMapperFiles(trainData,testData,fs,localDirStr,i,null);
        files.clear();
        trainData.clear();
    }
} 

&lt;h2 id=&#34;step-1-train-forests&#34;&gt;Step 1 &amp;mdash;Train forests&lt;/h2&gt;
// Create the Random Forest classifier that uses 5 neighbors to make decisions
Classifier rf = new RandomForest(numTrees, false, numAttributes, new Random());
// Learn the forest
rf.buildClassifier(trainDataPoints);

&lt;h2 id=&#34;step-2-synchronize-majority-vote&#34;&gt;Step 2 &amp;mdash; Synchronize majority vote&lt;/h2&gt;
for (Instance inst : testDataPoints) {
    // This will hold this random forest&#39;s class vote for this data point
    //IntArray votes = IntArray.create(C, false);
    IntArray votes = IntArray.create(RandomForestConstants.NUM_CLASSES, false);
    // Get the prediction class from this Random Forest
    Object predictedClassValue = rf.classify(inst);
    // Get the true value
    Object realClassValue = inst.classValue();
    //int predIndex = Integer.parseInt(preds.get(dp));
    // Check which class was predicted
    for (int i = 0; i &lt; RandomForestConstants.NUM_CLASSES; i++) {
        // Check to see if this index matches the class that was predicted
        if (predictedClassValue.equals(Integer.toString(i))) {
            // log.info(&#34;i: &#34; + i + &#34;; predictedClassValue: &#34; + predictedClassValue + &#34;; condition: &#34; + predictedClassValue.equals(Integer.toString(i)));
            votes.get()[i] = 1;
        } else {
            votes.get()[i] = 0;
        }
    }
    // Add the voting results to the partition
    Partition&lt;IntArray&gt; dpP = new Partition&lt;IntArray&gt;(dp, votes);
    predTable.addPartition(dpP);
    // Move onto the next data point
    dp++;
}
log.info(&#34;Done populating predTable\n&#34;);
// All Reduce from all Mappers
log.info(&#34;Before allreduce!!!!&#34;);
allreduce(&#34;main&#34;, &#34;allreduce&#34;, predTable);
log.info(&#34;After allreduce!!!!&#34;);
</description>
    </item>
    
    <item>
      <title>Harp Resources</title>
      <link>https://dsc-spidal.github.io/harp/docs/resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/resources/</guid>
      <description>

&lt;p&gt;Harp Resources outside this documentation:&lt;/p&gt;

&lt;h2 id=&#34;conference-journal-papers&#34;&gt;Conference &amp;amp; Journal Papers&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://ipcc.soic.iu.edu/Computation%20Abstractions.pdf&#34;&gt;B. Zhang, B. Peng, J. Qiu. Model-Centric Computation Abstractions in Machine Learning Applications&lt;/a&gt; (BeyondMR, 2016)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://ipcc.soic.iu.edu/ICCS-harp-lda.pdf&#34;&gt;B. Zhang, B. Peng, J. Qiu. High Performance LDA through Collective Model Communication Optimization&lt;/a&gt; (ICCS, 2016)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://ipcc.soic.iu.edu/A%20Collective%20Communication%20Layer.pdf&#34;&gt;B. Zhang. A Collective Communication Layer for the Software Stack of Big Data Analytics&lt;/a&gt; (Doctor symposium in IC2E, 2016)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://dsc.soic.indiana.edu/publications/HarpQiuZhang.pdf&#34;&gt;B. Zhang, Y. Ruan, J. Qiu. Harp: Collective Communication on Hadoop&lt;/a&gt; (Short paper in IC2E, 2015)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cgl.soic.indiana.edu/publications/116-zhang.pdf&#34;&gt;B. Zhang, J. Qiu. High Performance Clustering of Social Images in a Map-Collective Programming Model&lt;/a&gt; (Poster in SoCC, 2013)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;press&#34;&gt;Press&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://grids.ucs.indiana.edu/ptliupages/publications/MammothDataintheCloudClusteringSocialImages.pdf&#34;&gt;J. Qiu, B. Zhang. Mammoth Data in the Cloud: Clustering Social Images&lt;/a&gt;  (Book chapter in Cloud Computing and Big Data, IOS Press, 2013)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Harp Support Vector Machine</title>
      <link>https://dsc-spidal.github.io/harp/docs/examples/svm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/examples/svm/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/4-4-1.png&#34; width=&#34;50%&#34;  &gt;&lt;/p&gt;

&lt;p&gt;In machine learning, support vector machines (SVM) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each is marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and are predicted to belong to a category based on which side of the gap they fall.&lt;/p&gt;

&lt;p&gt;In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.&lt;/p&gt;

&lt;p&gt;In this project, Harp won&amp;rsquo;t touch the core code base of computing SVM. It will use LibSVM, which is an open source library and does parallel around LibSVM. And it is developed at the National Taiwan University and written in C++ with other programming languages&amp;rsquo; APIs. LibSVM implements the SMO algorithm for kernelized support vector machines (SVMs), supporting classification and regression.&lt;/p&gt;

&lt;h2 id=&#34;method&#34;&gt;METHOD&lt;/h2&gt;

&lt;p&gt;The Harp based SVM algorithm works as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The training set of the algorithm is split into subsets.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each node trains sub dataset locally via LibSVM&amp;rsquo;s API.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Allgather support vectors to global.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Each node combines its training data and the global support vectors.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat Step 2 to 4 until support vectors don&amp;rsquo;t change any more.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/4-4-2.png&#34; alt=&#34;SVM-2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The source code can be found in Harp GitHub repository by click &lt;a href=&#34;https://github.com/DSC-SPIDAL/harp/tree/master/harp-tutorial-app/src/main/java/edu/iu/svm&#34;&gt;Harp SVM&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;step-0-data-preprocessing&#34;&gt;Step 0 &amp;mdash; Data preprocessing&lt;/h2&gt;

&lt;p&gt;Harp SVM will follow LibSVM&amp;rsquo;s data format. Each data point in a file is represented by a line of the format &lt;code&gt;&amp;lt;label&amp;gt; [&amp;lt;fid&amp;gt;:&amp;lt;feature&amp;gt;]&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;label&amp;gt;&lt;/code&gt; which is 1 or -1&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;fid&amp;gt;&lt;/code&gt; is a positive feature id&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;feature&amp;gt;&lt;/code&gt; is the feature value&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After preprocessing, push the data set into HDFS by the following commands.&lt;/p&gt;
hdfs dfs -mkdir /input
hdfs dfs -put input_data/* /input

&lt;h2 id=&#34;step-1-initialize-and-load-data&#34;&gt;Step 1 &amp;mdash; Initialize and load data&lt;/h2&gt;
Vector&lt;Double&gt; vy = new Vector&lt;Double&gt;();
Vector&lt;svm_node[]&gt; vx = new Vector&lt;svm_node[]&gt;();
Vector&lt;Double&gt; svy = new Vector&lt;Double&gt;();
Vector&lt;svm_node[]&gt; svx = new Vector&lt;svm_node[]&gt;();
int maxIndex = 0;

//read data from HDFS
for (String dataFile : dataFiles) {
    FileSystem fs = FileSystem.get(configuration);
    Path dataPath = new Path(dataFile);
    FSDataInputStream in = fs.open(dataPath);
    BufferedReader br = new BufferedReader(new InputStreamReader(in));
    String line = &#34;&#34;;
    while ((line = br.readLine()) != null) {
        StringTokenizer st = new StringTokenizer(line,&#34; \t\n\r\f:&#34;);
        vy.addElement(Double.valueOf(st.nextToken()).doubleValue());
        int m = st.countTokens() / 2;
        svm_node[] x = new svm_node[m];
        for (int i = 0;i &lt; m;i++) {
            x[i] = new svm_node(); 
            x[i].index = Integer.parseInt(st.nextToken());
            x[i].value = Double.valueOf(st.nextToken()).doubleValue();
        }
        if (m &gt; 0) maxIndex = Math.max(maxIndex,x[m - 1].index);
        vx.addElement(x);
    }
    br.close();
}

HashSet&lt;String&gt; originTrainingData = new HashSet&lt;String&gt;();
for (int i = 0;i &lt; vy.size();i++) {
    String line = &#34;&#34;;
    line += vy.get(i) + &#34; &#34;;
    for (int j = 0;j &lt; vx.get(i).length - 1;j++) {
        line += vx.get(i)[j].index + &#34;:&#34; + vx.get(i)[j].value + &#34; &#34;;
    }
    line += vx.get(i)[vx.get(i).length - 1].index + &#34;:&#34; + vx.get(i)[vx.get(i).length - 1].value;
    originTrainingData.add(line);
}

//initial svm paramter
svmParameter = new svm_parameter();
svmParameter.svm_type = svm_parameter.C_SVC;
svmParameter.kernel_type = svm_parameter.RBF;
svmParameter.degree = 3;
svmParameter.gamma = 0;
svmParameter.coef0 = 0;
svmParameter.nu = 0.5;
svmParameter.cache_size = 100;
svmParameter.C = 1;
svmParameter.eps = 1e-3;
svmParameter.p = 0.1;
svmParameter.shrinking = 1;
svmParameter.probability = 0;
svmParameter.nr_weight = 0;
svmParameter.weight_label = new int[0];
svmParameter.weight = new double[0];

&lt;h2 id=&#34;step-2-train-via-libsvm-s-api&#34;&gt;Step 2 &amp;mdash;Train via LibSVM&amp;rsquo;s API&lt;/h2&gt;
//initial svm problem
svmProblem = new svm_problem();
svmProblem.l = currentTrainingData.size();
svmProblem.x = new svm_node[svmProblem.l][];
svmProblem.y = new double[svmProblem.l];
int id = 0;
for (String line : currentTrainingData) {
    StringTokenizer st = new StringTokenizer(line,&#34; \t\n\r\f:&#34;);
    svmProblem.y[id] = Double.valueOf(st.nextToken()).doubleValue();
    int m = st.countTokens() / 2;
    svm_node[] x = new svm_node[m];
    for (int i = 0;i &lt; m;i++) {
        x[i] = new svm_node(); 
        x[i].index = Integer.parseInt(st.nextToken());
        x[i].value = Double.valueOf(st.nextToken()).doubleValue();
    }
    svmProblem.x[id] = x;
    id++;
}

//compute model
svmModel = svm.svm_train(svmProblem, svmParameter);

&lt;h2 id=&#34;step-3-communication-among-nodes&#34;&gt;Step 3 &amp;mdash; Communication among nodes&lt;/h2&gt;
Table&lt;HarpString&gt; svTable = new Table(0, new HarpStringPlus());

HarpString harpString = new HarpString();
harpString.s = &#34;&#34;;
for (int i = 0;i &lt; svmModel.l;i++) {
    harpString.s += svmProblem.y[svmModel.sv_indices[i] - 1] + &#34; &#34;;
    for (int j = 0;j &lt; svmModel.SV[i].length - 1;j++) {
        harpString.s += svmModel.SV[i][j].index + &#34;:&#34; + svmModel.SV[i][j].value + &#34; &#34;;
    }
    harpString.s += svmModel.SV[i][svmModel.SV[i].length - 1].index + &#34;:&#34; + svmModel.SV[i][svmModel.SV[i].length - 1].value + &#34;\n&#34;;
}
Partition&lt;HarpString&gt; pa = new Partition&lt;HarpString&gt;(0, harpString);
svTable.addPartition(pa);

allreduce(&#34;main&#34;, &#34;allreduce_&#34; + iter, svTable);

supportVectors = new HashSet&lt;String&gt;();
String[] svString = svTable.getPartition(0).get().get().split(&#34;\n&#34;);
for (String line : svString) {
    if (!supportVectors.contains(line)) {
        supportVectors.add(line);
    }
}

&lt;h2 id=&#34;usage&#34;&gt;USAGE&lt;/h2&gt;

&lt;p&gt;Run Harp SVM:&lt;/p&gt;
$ hadoop jar build/harp3-app-hadoop-2.6.0.jar edu.iu.svm.IterativeSVM &lt;number of mappers&gt; &lt;number of iteration&gt; &lt;output path in HDFS&gt; &lt;data set path&gt;

&lt;p&gt;Fetch the result:&lt;/p&gt;
$ hdfs dfs -get &lt;output path in HDFS&gt; &lt;path you want to store the output&gt;
</description>
    </item>
    
    <item>
      <title>Harp-DAAL Framework</title>
      <link>https://dsc-spidal.github.io/harp/docs/harpdaal/harpdaal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dsc-spidal.github.io/harp/docs/harpdaal/harpdaal/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-1-1.png&#34; width=&#34;7.1775%&#34;&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-1-2.png&#34; width=&#34;51.3975%&#34;&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-1-3.png&#34; width=&#34;7.425%&#34;&gt;
&lt;br /&gt;
&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-2-1.png&#34; width=&#34;7.1775%&#34;&gt;&lt;a href=&#34;https://dsc-spidal.github.io/harp/docs/programming/computation-models/&#34;&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-2-2.png&#34; width=&#34;51.3975%&#34;&gt;&lt;/a&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-2-3.png&#34; width=&#34;7.425%&#34;&gt;
&lt;br /&gt;
&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-3-1.png&#34; width=&#34;7.1775%&#34;&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-3-2.png&#34; width=&#34;51.3975%&#34;&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-3-3.png&#34; width=&#34;7.425%&#34;&gt;
&lt;br /&gt;
&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-4-1.png&#34; width=&#34;7.1775%&#34;&gt;&lt;a href=&#34;https://software.intel.com/en-us/intel-daal&#34;&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-4-2.png&#34; width=&#34;51.3975%&#34;&gt;&lt;/a&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-4-3.png&#34; width=&#34;7.425%&#34;&gt;
&lt;br /&gt;
&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-5-1.png&#34; width=&#34;7.1775%&#34;&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-5-2.png&#34; width=&#34;51.3975%&#34;&gt;&lt;img src=&#34;https://dsc-spidal.github.io/harp/img/6-1-2/6-1-2-5-3.png&#34; width=&#34;7.425%&#34;&gt;&lt;/p&gt;

&lt;p&gt;Figure 1 shows the position of Harp-DAAL within the whole HPC-Big Data software stack.&lt;/p&gt;

&lt;h2 id=&#34;what-is-harp-daal&#34;&gt;What is Harp-DAAL?&lt;/h2&gt;

&lt;p&gt;Harp-DAAL is a new framework that aims to run data analytics algorithms on distributed HPC architectures. The framework consists of two layers: a communication layer and a computation layer. A communication layer is handled by Harp,
a communication library plug-in into Hadoop ecosystem, and a computation layer is handled by Intel&amp;rsquo;s Data Analytics Acceleration Library (DAAL), which is a library that provides
the users of well optimized building blocks for data analytics and machine learning applications on Intel&amp;rsquo;s architectures.&lt;/p&gt;

&lt;p&gt;Compared to contemporary communication libraries, such as Hadoop and Spark, Harp has the following advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MPI-like collective communication operations that are highly optimized for big data problems.&lt;/li&gt;
&lt;li&gt;efficient and innovative computation models for different machine learning problems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, the original Harp framework only supports development of Java applications, which is a common choice within the Hadoop ecosystem.
The downside of the pure Java implementation is the lack of support for emerging new hardware architectures such as Intel&amp;rsquo;s Xeon Phi.
By invoking DAAL&amp;rsquo;s native kernels, applications can leverage the huge number of threads on many-core platforms, which is a great
advantage for computation-intensive data analytics algorithms. This is also the tendency of merging HPC and Big Data domain.&lt;/p&gt;

&lt;h2 id=&#34;how-to-build-a-harp-daal-application&#34;&gt;How to build a Harp-DAAL Application ?&lt;/h2&gt;

&lt;p&gt;If you already have a legacy Harp application codes, you need only identify the local computation module, and replace it by invoking correspondent
DAAL kernels. Although DAAL&amp;rsquo;s kernels are written in C/C++, it does provide users of a Java API. The API is highly packaged, and the users only need
a few lines of codes to finish the invocation of kernels. For instance, the main function of a PCA application in DAAL is shown as below:&lt;/p&gt;
public static void main(String[] args) throws java.io.FileNotFoundException, java.io.IOException {

     /* Read a data set from a file and create a numeric table for storing the input data */
     CSRNumericTable data = Service.createSparseTable(context, datasetFileName);

     /* Create an algorithm to compute PCA decomposition using the correlation method */
     Batch pcaAlgorithm = new Batch(context, Double.class, Method.correlationDense);

     com.intel.daal.algorithms.covariance.Batch covarianceSparse
         = new com.intel.daal.algorithms.covariance.Batch(context, Double.class, com.intel.daal.algorithms.covariance.Method.fastCSR);
     pcaAlgorithm.parameter.setCovariance(covarianceSparse);

     /* Set the input data */
     pcaAlgorithm.input.set(InputId.data, data);

     /* Compute PCA decomposition */
     Result res = pcaAlgorithm.compute();

     NumericTable eigenValues = res.get(ResultId.eigenValues);
     NumericTable eigenVectors = res.get(ResultId.eigenVectors);
     Service.printNumericTable(&#34;Eigenvalues:&#34;, eigenValues);
     Service.printNumericTable(&#34;Eigenvectors:&#34;, eigenVectors);

     context.dispose();
}

&lt;p&gt;DAAL&amp;rsquo;s Java API is usually contains the following objects:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Data: user&amp;rsquo;s data packed in DAAL&amp;rsquo;s data structure, e.g., NumericTable, DataCollection&lt;/li&gt;
&lt;li&gt;Algorithm:  the engine of machine learning, each has three modes: Batch, Distri, Online&lt;/li&gt;
&lt;li&gt;Input: the user&amp;rsquo;s input data to Algorithm&lt;/li&gt;
&lt;li&gt;Parameter: the parameters provided by users during the running of algorithms&lt;/li&gt;
&lt;li&gt;Result: the feedback of Algorithm after running, retrieved by users&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before invoking your DAAL kernels,the most suitable data structure for the problem should be chosen. For many NumericTable types,
the Java API provides two ways of storing data. One is to store data on the JVM heap side, and whenever the native computation kernels require
the dataset, it will automatically copy the data from JVM heap to the off-heap memory space. The other way is to store data on Java&amp;rsquo;s direct byte buffer, and
native computation kernels can access them directly without any data copy. Therefore, you should evaluate the overhead of loading and writing data from memory
in your application. For many data-intensive applications, it is wise to store the data on the direct byte buffer.&lt;/p&gt;

&lt;p&gt;If you build the Harp-DAAL application from scratch, you should also carefully choose the data structure on the Harp side. The thumb rule is to allocate data in
contiguous primitive Java array, because most of DAAL&amp;rsquo;s Java API only accepts primitive array as input arguments. If Harp&amp;rsquo;s own Table structure is used and the
contained data is distributed into different partitions, then you may use the Harp-DAAL data conversion API to transfer the data between a Harp table and a DAAL
table.&lt;/p&gt;

&lt;h3 id=&#34;harp-daal-data-conversion-api&#34;&gt;Harp-DAAL Data Conversion API&lt;/h3&gt;

&lt;p&gt;Harp-DAAL now provides a group of classes under the path &lt;em&gt;Harp/harp-daal-app/src/edu/iu/daal&lt;/em&gt;, which manipulates the data transfer
between Harp&amp;rsquo;s data structure and that of DAAL.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RotatorDaal: a rotator which internally converts the H matrix from Harp table to DAAL&amp;rsquo;s NumericTable&lt;/li&gt;
&lt;li&gt;RotateTaskDaal: the tasks executed by RotatorDaal in the model rotation paradigm.&lt;/li&gt;
&lt;li&gt;HomogenTableHarpMap: convert data between DAAL&amp;rsquo;s HomogenNumericTable and Harp&amp;rsquo;s map&lt;/li&gt;
&lt;li&gt;HomogenTableHarpTable: convert data between DAAL&amp;rsquo;s HomogenNumericTable and Harp&amp;rsquo;s table&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Within the &lt;em&gt;RotatorDaal&lt;/em&gt;, the data transfers between Harp and DAAL is also overlapped by the computation work in another pipeline. Thus, if there is enough computation workload, the
overhead of data conversion could be significantly reduced. It is also very straightforward to invoke these conversion tools.&lt;/p&gt;

//create a conversion class between harp map and daal&#39;s table
HomogenTableHarpMap&lt;double[]&gt; converter = new HomogenTableHarpMap&lt;double[]&gt;(wMap, wMap_index, wMap_daal, wMap_size, r, numThreads);
convert_wTable.HarpToDaalDouble();

//create a conversion class between a harp table and a daal table
converter = new HomogenTableHarpTable&lt;I, P, Table&lt;P&gt; &gt;(table, this.daal_table, table.getNumPartitions(), this.rdim, this.numThreads);
converter.HarpToDaalDouble();


&lt;h2 id=&#34;how-to-compile-and-run-harp-daal-application&#34;&gt;How to Compile and Run Harp-DAAL Application ?&lt;/h2&gt;

&lt;h3 id=&#34;install-intel-s-daal&#34;&gt;Install Intel&amp;rsquo;s DAAL&lt;/h3&gt;

&lt;p&gt;You can either download Intel&amp;rsquo;s DAAL product with licence from their website &lt;a href=&#34;https://software.intel.com/en-us/blogs/daal&#34;&gt;https://software.intel.com/en-us/blogs/daal&lt;/a&gt;, or build it from source code. DAAL&amp;rsquo;s source code
is open-sourced and available on the GitHub.&lt;/p&gt;

git clone https://github.com/01org/daal.git


&lt;p&gt;After installation, you can run the bin/daalvars.sh script to set up all the DAAL related environment variables.&lt;/p&gt;

source /path-to-daal/bin/daalvars.sh intel64


&lt;p&gt;The important environment variable is the &lt;em&gt;DAALROOT&lt;/em&gt;, which points to the path of DAAL&amp;rsquo;s source code. You can run the examples of each algorithm within DAAL to test
the installation of your DAAL library.&lt;/p&gt;

cd $DAALROOT/../__release_lnx/daal/examples/cpp
make {libia32|soia32|libintel64|sointel64|help} [example=name] [compiler=compiler_name] [mode=mode_name] [threading=threading_name]


&lt;h3 id=&#34;setup-daal-within-harp&#34;&gt;Setup DAAL within Harp&lt;/h3&gt;

&lt;p&gt;To use DAAL within Harp, you need first add DAAL Java API to your Java source code&lt;/p&gt;

// packages from Daal
import com.intel.daal.services.DaalContext;
import com.intel.daal.algorithms.*;
import com.intel.daal.data_management.data.NumericTable;


&lt;p&gt;To compile your Harp-DAAL codes by ant, add the following lines to your &lt;em&gt;build.xml&lt;/em&gt;&lt;/p&gt;

&lt;fileset dir=&#34;${env.DAALROOT}&#34;&gt;
&lt;include name=&#34;**/*.jar&#34; /&gt;
&lt;include name=&#34;**/lib/*.jar&#34; /&gt;
&lt;/fileset&gt;


&lt;p&gt;Since DAAL&amp;rsquo;s Java API will invoke the native DAAL kernels, we need to load the native lib files into Harp&amp;rsquo;s environment in two steps&lt;/p&gt;

&lt;p&gt;1.Load these required native libraries in HDFS&lt;/p&gt;

hdfs dfs -put ${DAALROOT}/lib/intel64_lin/libJavaAPI.so /Hadoop/Libraries/
hdfs dfs -put ${DAALROOT}/../tbb/lib/intel64_lin/gcc4.4/libtbb* /Hadoop/Libraries/
hdfs dfs -put ${DAALROOT}/../../daal-misc/lib/libiomp5.so /Hadoop/Libraries/


&lt;p&gt;2.Load native libraries from HDFS to distributed cache of Harp&amp;rsquo;s program.&lt;/p&gt;

/* Put shared libraries into the distributed cache */
Configuration conf = this.getConf();
DistributedCache.createSymlink(conf);
DistributedCache.addCacheFile(new URI(&#34;/Hadoop/Libraries/libJavaAPI.so#libJavaAPI.so&#34;), conf);
DistributedCache.addCacheFile(new URI(&#34;/Hadoop/Libraries/libtbb.so.2#libtbb.so.2&#34;), conf);
DistributedCache.addCacheFile(new URI(&#34;/Hadoop/Libraries/libtbb.so#libtbb.so&#34;), conf);
DistributedCache.addCacheFile(new URI(&#34;/Hadoop/Libraries/libtbbmalloc.so.2#libtbbmalloc.so.2&#34;), conf);
DistributedCache.addCacheFile(new URI(&#34;/Hadoop/Libraries/libtbbmalloc.so#libtbbmalloc.so&#34;), conf);
DistributedCache.addCacheFile(new URI(&#34;/Hadoop/Libraries/libiomp5.so#libiomp5.so&#34;), conf);

</description>
    </item>
    
  </channel>
</rss>