<!DOCTYPE html><html lang="en-us"><head><link rel="shortcut icon" type="image/png" sizes="16x16" href="https://dsc-spidal.github.io/harp/img/favicon-16x9.png"><link rel="shortcut icon" type="image/png" sizes="32x32" href="https://dsc-spidal.github.io/harp/img/favicon-32x18.png"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Hugo 0.18" /><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="harp"><meta property="og:image" content="/img/logo54x54.png"><meta property="og:url" content="https://dsc-spidal.github.io/harp/"><meta name="twitter:title" content="harp"><meta name="twitter:image" content="/img/0-1-1.png"><meta property="og:site_name" content="Harp-DAAL-SGD"><meta property="og:url" content="/docs/harpdaal/mfsgd/"><meta property="og:description" content=""><meta name="twitter:description" content=""><title>harp Documentation - Harp-DAAL-SGD</title><link rel="stylesheet" href="https://dsc-spidal.github.io/harp/css/style.min.css"><link rel="stylesheet" href="https://dsc-spidal.github.io/harp/css/font-awesome.min.css"><link rel="stylesheet" href="https://dsc-spidal.github.io/harp/css/pygments.css"></head><body><nav class="hn-top-navbar navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="hn-navbar-container container-fluid"><div class="navbar-header"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#hn-navbar" aria-controls="hn-navbar" aria-expanded="false"><span class="sr-only"><Toggle>navigation</Toggle></span><i class="hn-toggle-button fa fa-bars"></i></button><a class="hn-navbar-logo navbar-brand" href="https://dsc-spidal.github.io/harp/"><img class="pull-left" src="https://dsc-spidal.github.io/harp/img/0-1-2.png"></a></div><div id="hn-navbar" class="navbar-collapse collapse"><ul class="nav navbar-nav navbar-right"><li><a href="https://dsc-spidal.github.io/harp/docs/getting-started">Docs</a></li><li><a href="https://dsc-spidal.github.io/harp/api">API</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/resources">Resources</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/contributors/community">Community</a></li><li><a href="https://github.com/DSC-SPIDAL/harp/">GitHub</a></li><li><a href="https://groups.google.com/forum/#!forum/harp-users">Mailing List</a></li></ul></div></div></nav><div class="hn-main"><div class="container"><div class="row"><aside class="hn-sidebar hidden-xs col-sm-4 col-md-3 col-lg-2 collapse"><nav class="hn-sidebar-nav"><div id="hn-accordion" class="panel-group" role="tablist" aria-multiselectable="true"><div class="panel panel-default"><section id="quick-start" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-quick-start" aria-labelledby="quick-start"><i class="fa fa-caret-right"></i>Quick Start</a></h4></section><div id="collapse-quick-start" class="panel-collapse collapse" aria-labelledby="quick-start"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp/docs/getting-started">Harp Installation (single)</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/getting-started-cluster">Harp Installation (cluster)</a></li></ul></div></div></div><div class="panel panel-default"><section id="programming-guides" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-programming-guides" aria-labelledby="programming-guides"><i class="fa fa-caret-right"></i>Programming Guides</a></h4></section><div id="collapse-programming-guides" class="panel-collapse collapse" aria-labelledby="programming-guides"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp/docs/programming/overview">Overview</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/programming/data-interface">Data Interfaces and Types</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/programming/scheduler">Schedulers</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/programming/computation-models">Computation Models</a></li></ul></div></div></div><div class="panel panel-default"><section id="collective-communication" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-collective-communication" aria-labelledby="collective-communication"><i class="fa fa-caret-right"></i>Collective Communication</a></h4></section><div id="collapse-collective-communication" class="panel-collapse collapse" aria-labelledby="collective-communication"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp/docs/communications/broadcast">broadcast</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/communications/reduce">reduce</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/communications/allgather">allgather</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/communications/allreduce">allreduce</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/communications/regroup">regroup</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/communications/pushandpull">push &amp; pull</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/communications/rotate">rotate</a></li></ul></div></div></div><div class="panel panel-default"><section id="examples" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-examples" aria-labelledby="examples"><i class="fa fa-caret-right"></i>Examples</a></h4></section><div id="collapse-examples" class="panel-collapse collapse" aria-labelledby="examples"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp/docs/examples/overview">Overview</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/examples/kmeans">K-Means</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/examples/mlrsgd">Multiclass Logistic Regression</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/examples/lda">Latent Dirichlet Allocation (CVB)</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/examples/svm">Support Vector Machine</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/examples/rf">Random Forests</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/examples/nn">Neural Network</a></li></ul></div></div></div><div class="panel panel-default"><section id="applications" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-applications" aria-labelledby="applications"><i class="fa fa-caret-right"></i>Applications</a></h4></section><div id="collapse-applications" class="panel-collapse collapse" aria-labelledby="applications"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp/docs/applications/lda-cgs">Latent Dirichlet Allocation (CGS)</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/applications/mf">Matrix Factorization</a></li></ul></div></div></div><div class="panel panel-default"><section id="harp-daal" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-harp-daal" aria-labelledby="harp-daal"><i class="fa fa-caret-right"></i>Harp-DAAL</a></h4></section><div id="collapse-harp-daal" class="panel-collapse collapse" aria-labelledby="harp-daal"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp/docs/harpdaal/harpdaal">Overview</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/harpdaal/mfsgd">Matrix Factorization (SGD)</a></li></ul></div></div></div><div class="panel panel-default"><section id="harp-resources" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-harp-resources" aria-labelledby="harp-resources"><i class="fa fa-caret-right"></i>Harp Resources</a></h4></section><div id="collapse-harp-resources" class="panel-collapse collapse" aria-labelledby="harp-resources"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp/docs/resources">Harp Resources</a></li></ul></div></div></div><div class="panel panel-default"><section id="contributors" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-contributors" aria-labelledby="contributors"><i class="fa fa-caret-right"></i>Contributors</a></h4></section><div id="collapse-contributors" class="panel-collapse collapse" aria-labelledby="contributors"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp/docs/contributors/community">Community</a></li><li><a href="https://dsc-spidal.github.io/harp/docs/contributors/contributors">Contributors</a></li></ul></div></div></div></div></nav></aside><section class="hn-docs-main col-sm-8 col-md-9 col-lg-10 col-sm-offset-4 col-md-offset-3 col-lg-offset-2"><header class="hn-docs-header page-header"><h1>Harp-DAAL-SGD</h1><div class="hn-docs-description"></div></header><article class="hn-docs-content">

<p><img src="https://dsc-spidal.github.io/harp/img/6-2-6.png" width="70%"  ></p>

<h2 id="matrix-factorization-based-on-stochastic-gradient-descent-mf-sgd">Matrix Factorization based on Stochastic Gradient Descent (MF-SGD)</h2>

<p>Matrix Factorization based on Stochastic Gradient Descent (MF-SGD for short) is an algorithm widely used in recommender systems.
It aims to factorize a sparse matrix into two low-rank matrices named mode W and model H as follows.</p>

<p><img src="https://dsc-spidal.github.io/harp/img/6-2-1.png" width="10%" height="10%"><br></p>

<p>The rating Matrix V includes both training data and test data. A learning algorithm uses the training data to update matrices W and H.
For instance, a standard SGD procedure will update the model W and H
while iterating over each training data point, i.e., an entry in matrix V in the following formula.</p>

<p><img src="https://dsc-spidal.github.io/harp/img/6-2-2.png" width="25%" height="25%"><br></p>

<p><img src="https://dsc-spidal.github.io/harp/img/6-2-3.png" width="40%" height="40%"><br></p>

<p><img src="https://dsc-spidal.github.io/harp/img/6-2-4.png" width="40%" height="40%"><br></p>

<p>After the training process, the test data points in matrix V could be used to verify the effectiveness of the training matrix by computing the RMSE values of
the difference</p>

<p><img src="https://dsc-spidal.github.io/harp/img/6-2-2.png" width="25%" height="25%"><br></p>

<h2 id="implementation-of-sgd-within-harp-daal-framework">Implementation of SGD within Harp-DAAL Framework</h2>

<p>Harp-DAAL-SGD inherits the model-rotation computation model from Harp-SGD. It owns two layers: 1) an inter-mapper layer that decomposes the original MF-SGD problem into different
Harp Mappers. 2) an intra-mapper layer that carries out the computation work on local training data in a multi-threading paradigm.</p>

<h3 id="inter-mapper-layout">Inter-Mapper Layout</h3>

<p>The training dataset is partitioned by row identities, and each mapper is assigned data points from a group of rows.
The model matrix W is also row-partitioned, and each mapper keeps its own local portion of W. The model H is, however, sliced and rotated among all the mappers. Figure 1 shows the
inter-mapper layout of Harp-DAAL-SGD.</p>

<p><img src="https://dsc-spidal.github.io/harp/img/6-2-5.png" width="66%"  ></p>

<h3 id="intra-mapper-layout">Intra-Mapper Layout</h3>

<p>In each iteration, a mapper receives a slice of model H, i.e., a group of columns from matrix H. A procedure will pick out the training data points with column identities from these columns and
execute an updating task according to the SGD algorithm. Unlike the model-rotation model, the intra-mapper layer chooses the asynchronous computation model, where each training data point
updates its own rows from model matrices W and H without mutual locks.</p>

<p>For the intra-mapper parallel computing, we adopt a hybrid usage of TBB concurrent containers and OpenMP directives.</p>

<h2 id="a-code-walk-through-of-harp-daal-sgd">A Code Walk through of Harp-DAAL-SGD</h2>

<p>The main body of Harp-DAAL-SGD is the <em>mapCollective</em> function of class <em>SGDDaalCollectiveMapper</em>.</p>

protected void mapCollective(KeyValReader reader, 
            Context context) throws IOException, InterruptedException {

            LinkedList<String> vFiles = getVFiles(reader);

            try {
                runSGD(vFiles, context.getConfiguration(), context);
            } catch (Exception e) {
                LOG.error("Fail to run SGD.", e);
            }
}


<p>It first uses the function <em>getVFiles</em> to read in HDFS files. Then, it runs the <em>runSGD</em> to finish the iterative training process. Besides the <em>vFiles</em>, <em>runSGD</em> will also
take in the configurations of all the parameters that are required by the training and testing process. The following list includes some of the important parameters.</p>

<ul>
<li>r: the feature dimension of model data</li>
<li>lambda: the lambda parameter in the formula of updating model W and H</li>
<li>epsilon: the learning rate in the formula of updating model W and H</li>
<li>numIterations: the number of iterations in the training process</li>
<li>numThreads: the number of threads used in Java multi-threading programming and TBB</li>
<li>numModelSlices: the number of pipelines in model rotation</li>
</ul>

<p>The function <em>runSGD</em> contains several steps as follows:</p>

<h3 id="loading-training-and-testing-datasets-from-hdfs">Loading Training and Testing Datasets from HDFS</h3>

<p>First it invokes class <em>SGDUtil</em> to load datasets</p>

//----------------------- load the train dataset-----------------------
Int2ObjectOpenHashMap<VRowCol> vRowMap = SGDUtil.loadVWMap(vFilePaths, numThreads, configuration);

//-----------------------load the test dataset-----------------------
Int2ObjectOpenHashMap<VRowCol> testVColMap = SGDUtil.loadTestVHMap(testFilePath, configuration, numThreads);


<h3 id="regrouping-training-dataset-and-load-data-into-daal">Regrouping Training Dataset and Load Data into DAAL</h3>

<p>The second step is to re-organize the training dataset among mappers, thus, each mapper will get a portion of data points on a group of rows.
Harp provides the following interface for regrouping data:</p>

regroup("sgd", "regroup-vw", vSetTable, new Partitioner(this.getNumWorkers()));


<p><em>vSetTable</em> is a harp container that consists of different partitions. Here, each partition is an array of training data points with the same row identity.
After each mapper gets its proper quote of training data, it starts to load the training data into DAAL&rsquo;s data container. We use the <em>NumericTable</em> container of
DAAL, and its interface receives Java data in a primitive array type. The conversion takes two steps of data copy.</p>

<ul>
<li>Copy each partition of vSetTable into a single primitive array of data.</li>
<li>Copy the primitive array from JVM heap memory into Off-JVM heap memory.</li>
</ul>

<p>The data copy in the first step is done in parallel by using Java thread package.</p>

 train_wPos_daal = new HomogenBMNumericTable(daal_Context, Integer.class, 1, workerNumV, NumericTable.AllocationFlag.DoAllocate);
 train_hPos_daal = new HomogenBMNumericTable(daal_Context, Integer.class, 1, workerNumV, NumericTable.AllocationFlag.DoAllocate);
 train_val_daal = new HomogenBMNumericTable(daal_Context, Double.class, 1, workerNumV, NumericTable.AllocationFlag.DoAllocate);
 
 Thread[] threads = new Thread[numThreads];
 
 LinkedList<int[]> train_wPos_daal_sets = new LinkedList<>();
 LinkedList<int[]> train_hPos_daal_sets = new LinkedList<>();
 LinkedList<double[]> train_val_daal_sets = new LinkedList<>();
 
 for(int i=0;i<numThreads;i++)
 {
     train_wPos_daal_sets.add(new int[reg_tasks.get(i).getNumPoint()]);
     train_hPos_daal_sets.add(new int[reg_tasks.get(i).getNumPoint()]);
     train_val_daal_sets.add(new double[reg_tasks.get(i).getNumPoint()]);
 }
 
 for (int q = 0; q<numThreads; q++)
 {
     threads[q] = new Thread(new TaskLoadPoints(q, numThreads, reg_tasks.get(q).getSetList(),
                 train_wPos_daal_sets.get(q),train_hPos_daal_sets.get(q), train_val_daal_sets.get(q)));
 
     threads[q].start();
 }
 
 for (int q=0; q< numThreads; q++) {
 
     try
     {
         threads[q].join();
     }catch(InterruptedException e)
     {
         System.out.println("Thread interrupted.");
     }
 
 }


<p>The second step is done inside DAAL codes by using the <em>releaseBlockOfColumnValues</em> function from DAAL&rsquo;s Java API. This
function internally create a direct byte buffer to transfer the data.</p>

int itr_pos = 0;
for (int i=0;i<numThreads; i++)
{

    train_wPos_daal.releaseBlockOfColumnValues(0, itr_pos, reg_tasks.get(i).getNumPoint(), train_wPos_daal_sets.get(i));
    train_hPos_daal.releaseBlockOfColumnValues(0, itr_pos, reg_tasks.get(i).getNumPoint(), train_hPos_daal_sets.get(i));
    train_val_daal.releaseBlockOfColumnValues(0, itr_pos, reg_tasks.get(i).getNumPoint(), train_val_daal_sets.get(i));
    itr_pos += reg_tasks.get(i).getNumPoint();

}


<h3 id="create-model-matrices-and-model-rotator">Create Model Matrices and Model Rotator</h3>

<p>Two model matrices, W matrix and H matrix, are both the input data and output data. We initialize them with random values, and use them
after training to predict new data. Each mapper owns its portion of the whole W matrix, which is local to this mapper. This local W matrix is
thus stored at the Off-JVM heap memory space, which is accessible to the DAAL native kernels. We only transfer an array of row identities from
Java side into DAAL side, and the initialization is done within DAAL&rsquo;s kernel before the first iteration.</p>

//----------------- create the daal table for local row ids -----------------
wMat_size = idArray.size();
wMat_rowid_daal = new HomogenNumericTable(daal_Context, Integer.class, 1, wMat_size, NumericTable.AllocationFlag.DoAllocate);
wMat_rowid_daal.releaseBlockOfColumnValues(0, 0, wMat_size, ids);


<p>Unlike the W matrix, the H matrix is rotated among all the mappers multiple times in each iteration. Therefore, we keep one copy at the JVM heap memory and
another copy at the native off-JVM heap memory. The conversion of data between the harp table of H model and that of a DAAL container is handled by the
rotator class.</p>

// Create H model
Table<DoubleArray>[] hTableMap = new Table[numModelSlices];
createHModel(hTableMap, numModelSlices, vWHMap, oneOverSqrtR, random);
//create the rotator
RotatorDaal<double[], DoubleArray> rotator = new RotatorDaal<>(hTableMap, r, 20, this, null, "sgd");
rotator.start();


<p>As Harp-DAAL-SGD uses two pipelines to overlap the computation and communication work, the data conversion brought by the H model matrix is also likely to be
offset by the heavy computation work.</p>

<h3 id="local-computation-by-daal-kernels">Local Computation by DAAL Kernels</h3>

<p>We implemented the local DAAL codes in the MF-SGD-Distri algorithm of DAAL&rsquo;s repository. It is highly abstracted as the other DAAL&rsquo;s algorithms, and the users only
need a few lines of codes to invoke it.</p>

//create DAAL algorithm object, using distributed version of DAAL-MF-SGD
Distri sgdAlgorithm = new Distri(daal_Context, Double.class, Method.defaultSGD);

sgdAlgorithm.input.set(InputId.dataWPos, train_wPos_daal);
sgdAlgorithm.input.set(InputId.dataHPos, train_hPos_daal);
sgdAlgorithm.input.set(InputId.dataVal, train_val_daal);

sgdAlgorithm.input.set(InputId.testWPos, test_wPos_daal);
sgdAlgorithm.input.set(InputId.testHPos, test_hPos_daal);
sgdAlgorithm.input.set(InputId.testVal, test_val_daal);

PartialResult model_data = new PartialResult(daal_Context);
sgdAlgorithm.setPartialResult(model_data);

model_data.set(PartialResultId.presWMat, wMat_rowid_daal);


<p>The training and test dataset are imported to <em>sgdAlgorithm</em> as input arguments while the W matrix and H matrix are imported as result arguments. The kernel class is configurable with respect to the
precision, the internal algorithm, and so forth. The same <em>sgdAlgorithm</em> could be used in both of the training and test process.</p>

<p>First, we compute the RMSE value before the training process.</p>

printRMSEbyDAAL(sgdAlgorithm, model_data, rotator, numWorkers, totalNumTestV, wMat_size, 0, configuration);


<p>Second, we start the iterative training process loops.</p>

for (int i = 1; i <= numIterations; i++) {

    for (int j = 0; j < numWorkers; j++) {

        for (int k = 0; k < numModelSlices; k++) {

            //get the h matrix from the rotator
            NumericTable hTableMap_daal = rotator.getDaal_Table(k);
            model_data.set(PartialResultId.presHMat, hTableMap_daal);

            //set up the parameters for MF-DAAL-SGD
            sgdAlgorithm.parameter.set(epsilon,lambda, r, wMap_size, hPartitionMapSize, 1, numThreads, 0, 1);

            //computation 
            sgdAlgorithm.compute();

            //trigger the rotator after one time of computation
            rotator.rotate(k);

        }
    }
}


<p>After each iteration, we can choose to evaluate the training result immediately, or we may evaluate the result after every certain times of training iterations.</p>
</article></section></div></div></div><script src="https://code.jquery.com/jquery-2.2.1.min.js"></script><script src="https://dsc-spidal.github.io/harp/js/app.min.js"></script></body></html>